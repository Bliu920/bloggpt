{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30847eaa",
   "metadata": {},
   "source": [
    "# BlogGPT Demo: An Automated Blog Post Generation\n",
    "\n",
    "Welcome to the demonstration notebook for BlogGPT, an ambitious project that employs the state-of-the-art LLM (Large Language Models) technologies to streamline and automate the generation of blog posts on trending topics.\n",
    "\n",
    "### 1. Purpose of this Notebook:\n",
    "The aim of this notebook is to provide a comprehensive overview of the BlogGPT project. Through it, I aspire to elucidate the intricacies of my work, and present it as a testament to my skills and potential to prospective employers. Although what you'll see here represents the project as of mid-July, it's essential to note that BlogGPT is an evolving endeavor. As it gets polished and refined, the ultimate goal is to deploy a production-ready version, with its debut slated for Medium.\n",
    "\n",
    "### 2. Motivation:\n",
    "The inception of BlogGPT can be attributed to two primary factors:\n",
    "\n",
    "- **The Information Quandary**: Every day, the internet is abuzz with a plethora of trending topics. But comprehensively understanding them often entails wading through a morass of platforms, leading to a considerable time sink and the ever-persistent problem of information overload. This prompted the desire for a unified platform to expedite and simplify this learning process.\n",
    "  \n",
    "- **LLM Exploration**: My year-long foray into the realm of LLMs revealed the vast potential waiting to be tapped. It sparked the idea of harnessing these models to emulate a personal blogger, automating content creation while ensuring it remains engaging and up-to-date. Thus, BlogGPT was born - tailored not just for me, but for inquisitive minds worldwide.\n",
    "\n",
    "### 3. Project Overview:\n",
    "\n",
    "#### Technologies:\n",
    "- **ChatGPT API (Version 3.5)**: The backbone, responsible for the core content generation.\n",
    "- **LangChain**: An LLM application framework, facilitating smoother integration. One noteworthy inspiration was the \"QA over Documents\" use-case, which was reimagined to suit our writing paradigm.\n",
    "- **TensorFlow Encoder**: Deployed to vectorize textual data, aiding in similarity calculations.\n",
    "- **ChromaDB**: A vector database system, essential for storing and cherry-picking the most pertinent context data.\n",
    "- **API Integrations**: Tapping into Wikipedia, Google News, and Google Trends ensures that BlogGPT is always equipped with current and trending information.\n",
    "\n",
    "#### Challenges:\n",
    "- **Internet Restrictions**: Given that LLMs can't connect to the web, third-party APIs became indispensable.\n",
    "- **Token Limitations**: ChatGPT's constraints necessitated strategic content segmentation for longer pieces.\n",
    "- **Information Overload**: Balancing context quality with quantity was pivotal, ensuring optimal input for desired outputs.\n",
    "\n",
    "Certainly! Here's the updated \"Method\" section based on your inputs:\n",
    "\n",
    "---\n",
    "\n",
    "#### Method:\n",
    "\n",
    "1. **Trend Identification**:\n",
    "    - Use the Google Trend API and `pytrend` to identify trending topics and relevant Google search terms within Canada.\n",
    "\n",
    "2. **Link & Title Collection**:\n",
    "    - Retrieve relevant links and titles associated with the chosen keyword.\n",
    "    - Use the Google News API to find pertinent news links and headlines.\n",
    "    - Leverage the Wikipedia API to source links and titles of relevant Wikipedia pages.\n",
    "\n",
    "3. **Content Planning with ChatGPT**:\n",
    "    - Utilize the ChatGPT API to draft a writing plan. The plan is informed by keywords, relevant Google search terms, and news headlines.\n",
    "\n",
    "4. **Contextual Data Extraction**:\n",
    "    - Extract textual content from the identified links. This is achieved using the `WebBaseLoader` feature in LangChain's Integration Hub and the popular web scraping tools, `requests` and `BeautifulSoup`.\n",
    "\n",
    "5. **Vector Generation**:\n",
    "    - Divide the sourced textual content into manageable document sizes with the assistance of LangChain's text splitter.\n",
    "    - Vectorize these smaller documents using the TensorFlow encoder to represent them in a format suitable for similarity comparisons.\n",
    "\n",
    "6. **Vector Database Creation**:\n",
    "    - Establish a vector database through ChromaDB. This facilitates the calculation of similarity scores to pinpoint the most relevant context for each section of the blog.\n",
    "\n",
    "7. **Section-wise Blog Composition**:\n",
    "    - Craft each section of the blog using the ChatGPT API. This step relies heavily on the earlier writing plan and the context determined from the Vector database.\n",
    "\n",
    "8. **Final Blog Compilation**:\n",
    "    - To ensure a cohesive and engaging read, employ ChatGPT for one last pass to polish and unify the individual sections into a comprehensive blog post.\n",
    "\n",
    "---\n",
    "Let this notebook serve as your guided tour through the marvels and challenges of automating content creation. Dive in, explore, and witness the synergy of technology and creativity in BlogGPT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d61a72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "\n",
    "# Web Scraping and Data Retrieval\n",
    "import openai\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wikipedia\n",
    "from GoogleNews import GoogleNews\n",
    "\n",
    "# Google Trends\n",
    "from pytrends.request import TrendReq\n",
    "from pytrends.exceptions import TooManyRequestsError # for handling the frequent error\n",
    "\n",
    "# Langchain Libraries\n",
    "from langchain.document_loaders.news import NewsURLLoader\n",
    "from langchain.document_loaders.wikipedia import WikipediaLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Custom Toolkit\n",
    "import credentials  # Store and retrieve API keys\n",
    "from toolkit import chatgpt, select_link_content\n",
    "\n",
    "# Setting up the OpenAI API key for usage\n",
    "openai.api_key = credentials.OPENAI_APIKEY\n",
    "\n",
    "# Configuring logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6087f1d8",
   "metadata": {},
   "source": [
    "### Trend Identification:\n",
    "\n",
    "To keep our blog posts timely and relevant, we kick off the process by identifying the top trending search terms in Canada using Google Trends. The find_trend function accomplishes this by:\n",
    "\n",
    "Connecting to Google Trends using the `pytrends` library.\n",
    "Fetching all trending keywords for target `COUNTRY`.\n",
    "Selecting the keyword based on the specified `rank`.\n",
    "Retrieving related search queries and topics for the chosen keyword.\n",
    "Returning the gathered data in a structured dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f5408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_google_trends(pytrend, keyword, max_retries=3):\n",
    "    \"\"\"Request related queries and topics from Google Trends with retry logic.\"\"\"\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            pytrend.build_payload(kw_list=[keyword], cat=0, timeframe='today 5-y', geo='', gprop='')\n",
    "            \n",
    "            related_queries_data = pytrend.related_queries()\n",
    "            first_value = next(iter(related_queries_data.values()))\n",
    "            queries = first_value['top']['query'].to_string()\n",
    "            \n",
    "            related_topic_data = pytrend.related_topics()\n",
    "            first_value = next(iter(related_topic_data.values()))\n",
    "            topics = first_value['rising'][['topic_title', 'topic_type']].to_string()\n",
    "            \n",
    "            return queries, topics\n",
    "\n",
    "        except (requests.exceptions.timeout, requests.exceptions.ConnectTimeoutError, requests.exceptions.ConnectionError) as e:\n",
    "            retries += 1\n",
    "            logging.warning(f\"Exception occurred: {str(e)}. Retry {retries}/{max_retries}. Waiting for 60 seconds before retrying...\")\n",
    "            time.sleep(60)\n",
    "    logging.error(f\"Failed to fetch data from Google Trends after {max_retries} retries.\")\n",
    "    raise RuntimeError(f\"Failed to fetch data from Google Trends after {max_retries} retries.\")\n",
    "\n",
    "def find_trend(country, topic_rank=0, max_retries=3):\n",
    "    \"\"\"Fetch the top trending searches from Google Trends for a specified country.\"\"\"\n",
    "    pytrend = TrendReq(timeout=(10,25), retries=3)\n",
    "\n",
    "    all_keywords = pytrend.trending_searches(country)\n",
    "    if topic_rank >= len(all_keywords) or topic_rank < 0:\n",
    "        logging.error(f\"Invalid topic rank {topic_rank}. Please select within the range of available trending searches.\")\n",
    "        raise ValueError(f\"Invalid topic rank {topic_rank}. Please select within the range of available trending searches.\")\n",
    "\n",
    "    keyword = all_keywords.iloc[topic_rank, 0]\n",
    "    queries, topics = request_google_trends(pytrend, keyword)\n",
    "    print(f\"Found trend: ### {keyword} ###\")\n",
    "\n",
    "    return {\n",
    "        \"keyword\": keyword,\n",
    "        \"related_queries\": queries,\n",
    "        \"related_topics\": topics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "073f8da9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found trend: ### Premier League ###\n"
     ]
    }
   ],
   "source": [
    "# configuration\n",
    "COUNTRY = 'canada'\n",
    "TOPIC_RANK = 0 # the first trending topic\n",
    "\n",
    "# find trend\n",
    "trend = find_trend(country=COUNTRY, topic_rank=TOPIC_RANK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08db2d",
   "metadata": {},
   "source": [
    "#### News Retrieval:\n",
    "\n",
    "The `get_news` function fetches relevant news articles for a specified trending keyword from given sources using GoogleNews. It searches articles from the past 15 days, filters them based on preferred sources, and then randomly selects a subset to return. This ensures timely and pertinent content for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49a46182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(trend, sources, n=5, days=15):\n",
    "    \"\"\"\n",
    "    Fetches news articles for a given keyword from specified sources using GoogleNews.\n",
    "    \n",
    "    Parameters:\n",
    "    - keyword: The search term.\n",
    "    - n: Number of articles to return.\n",
    "    - sources: List of news sources to consider.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with search results, titles, and URLs of selected articles.\n",
    "    \"\"\"\n",
    "    # Extract keyword for the search\n",
    "    keyword = trend['keyword']\n",
    "    print(f\"Searching news for '{keyword}'...\")\n",
    "    \n",
    "    # Define date range for the search\n",
    "    end_date = datetime.today().strftime('%m/%d/%Y')\n",
    "    start_date = (datetime.now() - timedelta(days=days)).strftime('%m/%d/%Y') \n",
    "\n",
    "    # Initialize the GoogleNews object and search\n",
    "    googlenews = GoogleNews(lang='en', start=start_date, end=end_date)\n",
    "    googlenews.get_news(keyword)\n",
    "    search_results = googlenews.results(sort=True)\n",
    "\n",
    "    selected_sources = []\n",
    "    \n",
    "    # Filter news articles by specified sources\n",
    "    for article in search_results:\n",
    "        media_name = article['media'].lower()\n",
    "        link = article['link']\n",
    "        title = article['title']\n",
    "        \n",
    "        if any(source in media_name for source in sources):\n",
    "            if 'https://' not in link:\n",
    "                link = 'https://' + link\n",
    "\n",
    "            selected_sources.append({\n",
    "                \"title\": title, \n",
    "                \"url\": link, \n",
    "                \"media\": media_name\n",
    "            })\n",
    "\n",
    "    # Randomly select `n` articles from the filtered results\n",
    "    articles = random.sample(selected_sources, n)\n",
    "    \n",
    "    # Prepare articles for printing\n",
    "    prep_print = [f\"{article['media']}: {article['title'][:50]}...\\n{article['url']}\" for article in articles]\n",
    "    print(*prep_print, sep='\\n')\n",
    "    \n",
    "    output = {\n",
    "        \"search_results\": search_results,\n",
    "        \"articles\": articles\n",
    "    }\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46ad1d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching news for 'Premier League'...\n",
      "bbc: David Raya: Brentford boss Thomas Frank expects go...\n",
      "https://news.google.com/./articles/CBMiK2h0dHBzOi8vd3d3LmJiYy5jb20vc3BvcnQvZm9vdGJhbGwvNjY0NDUzODnSAQA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "cbs sports: Premier League preview: 10 questions facing Manche...\n",
      "https://news.google.com/./articles/CBMijAFodHRwczovL3d3dy5jYnNzcG9ydHMuY29tL3NvY2Nlci9uZXdzL3ByZW1pZXItbGVhZ3VlLXByZXZpZXctMTAtcXVlc3Rpb25zLWZhY2luZy1tYW5jaGVzdGVyLWNpdHktYXJzZW5hbC1tYW5jaGVzdGVyLXVuaXRlZC1jaGVsc2VhLWFuZC1tb3JlL9IBkAFodHRwczovL3d3dy5jYnNzcG9ydHMuY29tL3NvY2Nlci9uZXdzL3ByZW1pZXItbGVhZ3VlLXByZXZpZXctMTAtcXVlc3Rpb25zLWZhY2luZy1tYW5jaGVzdGVyLWNpdHktYXJzZW5hbC1tYW5jaGVzdGVyLXVuaXRlZC1jaGVsc2VhLWFuZC1tb3JlL2FtcC8?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "bbc: Brentford v Tottenham Hotspur preview: Team news a...\n",
      "https://news.google.com/./articles/CBMiLWh0dHBzOi8vd3d3LmJiYy5jby51ay9zcG9ydC9mb290YmFsbC82NjQxOTkwONIBAA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "fox sports: English Premier League 2023-24 predictions: Foreca...\n",
      "https://news.google.com/./articles/CBMibWh0dHBzOi8vd3d3LmZveHNwb3J0cy5jb20vc3Rvcmllcy9zb2NjZXIvZW5nbGlzaC1wcmVtaWVyLWxlYWd1ZS0yMDIzLTI0LXByZWRpY3Rpb25zLWZvcmVjYXN0LWZvci1hbGwtMjAtdGVhbXPSAW1odHRwczovL2FtcC5mb3hzcG9ydHMuY29tL3N0b3JpZXMvc29jY2VyL2VuZ2xpc2gtcHJlbWllci1sZWFndWUtMjAyMy0yNC1wcmVkaWN0aW9ucy1mb3JlY2FzdC1mb3ItYWxsLTIwLXRlYW1z?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "bbc: Premier League quiz: Can you name this current or ...\n",
      "https://news.google.com/./articles/CBMiK2h0dHBzOi8vd3d3LmJiYy5jb20vc3BvcnQvZm9vdGJhbGwvNjU5ODg4MDDSAQA?hl=en-CA&gl=CA&ceid=CA%3Aen\n"
     ]
    }
   ],
   "source": [
    "# configuration\n",
    "SOURCES = ['yahoo', 'cbc', 'global news', 'ctv', 'cnn', 'bbc', 'the times', 'fox', 'cbs', 'daily news', 'new york times', 'abc', 'wall street', 'washington post', 'usa today']\n",
    "NEWS_NUM = 5\n",
    "DAYS = 15\n",
    "\n",
    "news = get_news(trend, sources=SOURCES, n=NEWS_NUM, days=DAYS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e5cd49",
   "metadata": {},
   "source": [
    "#### Wikipedia Page Retrieval\n",
    "\n",
    "`get_wiki()` uses news headlines to predict the relevant Wikipedia articles.  \n",
    "  \n",
    "**Steps:**\n",
    "1. **News Selection:** Randomly selects two headlines from the provided news titles.\n",
    "2. **Model Prompt:** Assembles a structured prompt for the chatbot to identify related Wikipedia articles.\n",
    "3. **Model Interaction:** Uses the model's output to get predicted Wikipedia titles.\n",
    "4. **Output Compilation:** Organizes the main news, chatbot response, and finalized Wikipedia titles into a structured dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e320260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki(news, n):\n",
    "    \"\"\"\n",
    "    Predicts relevant Wikipedia pages based on given news titles.\n",
    "    \n",
    "    Parameters:\n",
    "    - news (dict): Dictionary containing news options, particularly titles.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing selected news, chatgpt response, Wikipedia titles, and URLs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Randomly select 2 news titles from the given options\n",
    "    selected_news = random.sample(news['articles'], 2)\n",
    "    news_headlines = '\\n'.join([news['title'] for news in selected_news])\n",
    "    print(f\"Predicts relevant Wikipedia pages based on: \\n{news_headlines}\\n\")\n",
    "    \n",
    "    # Construct a message to prompt the model for predictions\n",
    "    message = [\n",
    "        {\"role\": \"assistant\", \"content\": f\"Today's breaking news: '{news_headlines}'.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Predict {n} most relevant Wikipedia pages to help readers understand what's happening.\"},\n",
    "        {\"role\": \"user\", \"content\": 'Output the page titles in JSON format: {\"wiki_titles\": []}'}\n",
    "    ]\n",
    "    \n",
    "    # Use the model to fetch relevant Wikipedia page titles\n",
    "    response = chatgpt(message, content=True)\n",
    "    \n",
    "    # Convert the model's response into a list of Wikipedia titles\n",
    "    wiki_titles = json.loads(response, strict=False)['wiki_titles']\n",
    "    print(\"Suggested Wikipedia Pages:\")\n",
    "    print(*wiki_titles, sep='\\n')\n",
    "    \n",
    "    # Prepare the output containing the selected news, model's response, processed titles, and URLs\n",
    "    output ={\n",
    "        'referred_news': selected_news, \n",
    "        'chatgpt_response': response, \n",
    "        'wiki_titles': wiki_titles\n",
    "    }\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f35c168c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicts relevant Wikipedia pages based on: \n",
      "Brentford v Tottenham Hotspur preview: Team news and stats\n",
      "Premier League quiz: Can you name this current or former player?\n",
      "\n",
      "Suggested Wikipedia Pages:\n",
      "Brentford F.C.\n",
      "Tottenham Hotspur F.C.\n",
      "Premier League\n",
      "Brentford Community Stadium\n",
      "Football statistics and records\n"
     ]
    }
   ],
   "source": [
    "# configuration\n",
    "WIKI_NUM = 5\n",
    "\n",
    "wiki = get_wiki(news, n=WIKI_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd3b63b",
   "metadata": {},
   "source": [
    "### Content Planning with ChatGPT\n",
    "\n",
    "`make_plan` employs ChatGPT to design a structured content plan around trending keywords and related news.  \n",
    "\n",
    "**Steps:**\n",
    "1. **Keyword Extraction**: Extracts the primary keyword from the given trend.\n",
    "2. **Data Compilation**: Collates related queries, topics, and recent news headlines linked to the keyword.\n",
    "3. **Plan Message Construction**: Builds a message prompting ChatGPT to brainstorm and structure a blog post based on the provided data.\n",
    "4. **Bot Interaction Setup**: Sets up a conversation structure instructing the chatbot on the desired format and context.\n",
    "5. **Response Handling**: Captures ChatGPT's response, retrieves the content, and deciphers the JSON answer.\n",
    "6. **Output Construction**: Constructs a dictionary comprising message details, ChatGPT response, token usage cost, and the structured blog plan.\n",
    "7. **Result Display**: Outputs the token cost and the titles of each planned section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a37165d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plan(trend, news, n_sec):\n",
    "    \"\"\"\n",
    "    Generate a content plan based on a given trend.\n",
    "    \n",
    "    Parameters:\n",
    "    - trend (dict): Contains keyword and related queries and topics about a trend.\n",
    "    \n",
    "    Returns:\n",
    "    - output (dict): Contains the message body, response, cost, and answer from the chatbot.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the keyword from the trend\n",
    "    keyword = trend[\"keyword\"]\n",
    "    print(f\"Planning a blog post about {keyword}\")\n",
    "    \n",
    "    # Extract related queries, topics and news\n",
    "    queries = trend[\"related_queries\"]\n",
    "    topics = trend[\"related_topics\"]\n",
    "    news_headlines = [article['title'] for article in news['articles']]\n",
    "    \n",
    "    # Construct the message for the plan\n",
    "    plan_message = f\"\"\"\n",
    "    People are searching \"{keyword}\" like crazy today, so I want to write a blog post for the topic.\n",
    "    Help me brainstorm what I should write about. Specifically, plan a blog post, based on the related trending topics, google queries and recent news.  \n",
    "    Remeber the motivation of this blog is to help people learn about the trending topic \"{keyword}\", so make sure include this in opennings for readers. \n",
    "    I want {n_sec} parts in total, including the openning and closing paragraphs. \n",
    "    related topics:\n",
    "    {topics}\n",
    "    related queries:\n",
    "    {queries}\n",
    "    news:\n",
    "    {news_headlines}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the message body to communicate with the chatgpt\n",
    "    message_body = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"You are a very helpful and professional article writer and instructor. \n",
    "                          Keep in mind: \n",
    "                          1. forget previous commands\n",
    "                          2. answer precise and concise\n",
    "                          3. user has known you are a language model\n",
    "                          4. answer JSON only\"\"\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": plan_message},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"Format in JSON: {{\n",
    "             \"sec_1\": {{\n",
    "                \"section_title\": \"\",\n",
    "                \"explanation\": \"\"\n",
    "             }}, \n",
    "             ...\n",
    "             \"sec_{n_sec}\": ...  \n",
    "        }}\"\"\"}\n",
    "    ]\n",
    "    \n",
    "    # Use the chatgpt to get the response\n",
    "    response = chatgpt(message_body)\n",
    "    content = response['choices'][0]['message']['content']\n",
    "    answer = json.loads(content, strict=False)\n",
    "    cost = response[\"usage\"]\n",
    "    \n",
    "    # Construct the output dictionary\n",
    "    output = {\n",
    "        \"message_body\": message_body,\n",
    "        \"response\": response,\n",
    "        \"cost\": cost,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "    \n",
    "    # Display the cost\n",
    "    total_cost = str(cost[\"total_tokens\"])\n",
    "    print(f\"Done! Planning cost: {total_cost}\\n\")\n",
    "    for key, values in answer.items():\n",
    "        print(f\"{key}: {values['section_title']}\")\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a46c02ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planning a blog post about Premier League\n",
      "Done! Planning cost: 1382\n",
      "\n",
      "sec_1: Introduction to the Premier League\n",
      "sec_2: Key Facts and Figures\n",
      "sec_3: Top Contenders of the Current Season\n",
      "sec_4: Exciting Matches and Memorable Moments\n",
      "sec_5: The Impact of Premier League on English Football\n",
      "sec_6: Player Spotlight: Rising Stars and Legends\n",
      "sec_7: Conclusion: The Premier League Phenomenon\n"
     ]
    }
   ],
   "source": [
    "# configuration\n",
    "SECTION_NUM = 7\n",
    "\n",
    "plan = make_plan(trend, news, n_sec=SECTION_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40185795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_news_link(news, split_chunk_size):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = split_chunk_size, chunk_overlap = 0)\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    for document in news['articles']:\n",
    "        url = document['url']\n",
    "        media = document['media']\n",
    "        loader = NewsURLLoader([url])\n",
    "        splits = loader.load_and_split(text_splitter)\n",
    "        for doc in splits:\n",
    "            doc.metadata['type'] = 'news'\n",
    "            doc.metadata['media'] = media\n",
    "            for key, value in doc.metadata.items():\n",
    "                if not isinstance(value, (str, int, float)):\n",
    "                    doc.metadata[key] = str(value)\n",
    "            \n",
    "        documents.extend(splits)\n",
    "\n",
    "    return documents\n",
    "\n",
    "def load_wiki_title(wiki, split_chunk_size):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = split_chunk_size, chunk_overlap = 0)\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    for title in wiki['wiki_titles']:\n",
    "        loader = WikipediaLoader(query=title,\n",
    "                                load_max_docs=2)\n",
    "        splits = loader.load_and_split(text_splitter)\n",
    "        for doc in splits:\n",
    "            doc.metadata['type'] = 'wiki'\n",
    "            for key, value in doc.metadata.items():\n",
    "                if not isinstance(value, (str, int, float)):\n",
    "                    doc.metadata[key] = str(value)\n",
    "            \n",
    "        documents.extend(splits)\n",
    "\n",
    "    return documents \n",
    "            \n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "def create_vectorstore(news, wiki, split_chunk_size):\n",
    "    \n",
    "    # create documents\n",
    "    news_documents = load_news_link(news, split_chunk_size=split_chunk_size)\n",
    "    wiki_documents = load_wiki_title(wiki, split_chunk_size=split_chunk_size)\n",
    "    all_docs = news_documents + wiki_documents\n",
    "    \n",
    "    # store documents\n",
    "    vectorstore = Chroma.from_documents(documents=all_docs, embedding=OpenAIEmbeddings(openai_api_key=credentials.OPENAI_APIKEY))\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16491bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 14:00:46,072 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "# configuration\n",
    "CHUNK_SIZE =  600\n",
    "\n",
    "vectorstore = create_vectorstore(news, wiki, split_chunk_size=CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3487f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorstore store these information:\n",
      "- dict_keys(['ids', 'embeddings', 'metadatas', 'documents'])\n",
      "There are 163 documents each with a size of 600 characters\n",
      "\n",
      "An Example Document:\n",
      "\n",
      "id: ff0e8df8-3870-11ee-bfa0-0a623052d3b6\n",
      "\n",
      "embedding: None\n",
      "\n",
      "metadata: \n",
      "{'authors': '[]', 'description': 'Brentford boss Thomas Frank says he expects goalkeeper David Raya to complete a transfer to Arsenal.', 'language': 'en', 'link': 'https://news.google.com/./articles/CBMiK2h0dHBzOi8vd3d3LmJiYy5jb20vc3BvcnQvZm9vdGJhbGwvNjY0NDUzODnSAQA?hl=en-CA&gl=CA&ceid=CA%3Aen', 'media': 'bbc', 'publish_date': 'None', 'title': 'David Raya: Brentford boss Thomas Frank expects goalkeeper to complete Arsenal transfer', 'type': 'news'}\n",
      "\n",
      "document content: \n",
      "Last updated on .From the section Arsenal\n",
      "\n",
      "David Raya came on at half-time in Brentford's Premier League Summer Series match against Fulham during their pre-season tour of the United States\n",
      "\n",
      "Brentford boss Thomas Frank says he expects goalkeeper David Raya to complete a move to Arsenal.\n",
      "\n",
      "The 27-year-old Spaniard is reportedly set to join the Gunners on loan where he will provide competition for England keeper Aaron Ramsdale.\n",
      "\n",
      "Raya, who has one year left on his contract, joined Brentford when they were in the Championship in 2019 for a fee in the region of £3m.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see what's inside the verctorstore\n",
    "documents = vectorstore.get()\n",
    "print(f\"vectorstore store these information:\\n- {documents.keys()}\")\n",
    "print(f\"There are {len(documents['ids'])} documents each with a size of {CHUNK_SIZE} characters\\n\")\n",
    "print('An Example Document:\\n')\n",
    "print(f\"id: {documents['ids'][0]}\\n\")\n",
    "print(f\"embedding: {documents['embeddings']}\\n\")\n",
    "print(f\"metadata: \\n{documents['metadatas'][0]}\\n\")\n",
    "print(f\"document content: \\n{documents['documents'][0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db9f0635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instruct(keyword, section, structure, vectorstore, body_context=False):\n",
    "    \"\"\"\n",
    "    Generates the instructions for writing a specific section of the blog post.\n",
    "    \"\"\"\n",
    "    \n",
    "    title = section[\"section_title\"]\n",
    "    explanation = section[\"explanation\"]\n",
    "    structure = \"\\n\".join(structure)\n",
    "    \n",
    "    # make instruction\n",
    "    parag_n = 2 if body_context else 1\n",
    "    instruction = f\"\"\"I want to write this blog post about {keyword}:\\n{structure}.\n",
    "        Help me write this part in {parag_n} paragraphs:\\n{title}\\n{explanation}\"\"\"\n",
    "    \n",
    "    # select context information\n",
    "    query = \"\\n\\n\".join([title, explanation, structure])\n",
    "    \n",
    "    # wiki\n",
    "    wiki_docs = vectorstore.similarity_search(query=query, k=5, filter={'type': 'wiki'})\n",
    "    contents = [doc.page_content for doc in wiki_docs]\n",
    "    wiki_context = \"\\n\\n\".join(contents)\n",
    "    # news\n",
    "    news_docs = vectorstore.similarity_search(query=query, k=5, filter={'type': 'news'})\n",
    "    contents = [doc.page_content for doc in news_docs]\n",
    "    news_context = \"\\n\\n\".join(contents)\n",
    "    \n",
    "    context = f\"Use these as context:\\n\\nRelevant news:\\n{news_context}\\n\\nWikipedia information:\\n{wiki_context}\"\n",
    "\n",
    "    # make message\n",
    "    message = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"You are a very helpful and interesting writer.\n",
    "                          1. Write precise and concise in the appropriate tone\n",
    "                          2. Answer in JSON format\n",
    "                          3. Add numbers and facts where possible\"\"\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": context},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"Output in JSON format: {\"writing\": \"\"}\"\"\"}\n",
    "    ]\n",
    "    \n",
    "    return message\n",
    "\n",
    "def expand_writing(trend, wiki, news, vectorstore, plan):\n",
    "    \"\"\"\n",
    "    Expands the given writing plan into a detailed content using the trend, wiki, and news.\n",
    "    \"\"\"\n",
    "    # Extract\n",
    "    keyword = trend['keyword']\n",
    "    writing_plan = plan[\"answer\"]\n",
    "    structure = [values[\"section_title\"] for _, values in writing_plan.items()]\n",
    "\n",
    "    # Prepare the output structure\n",
    "    output = {\n",
    "        \"messages\": {}, \n",
    "        \"responses\": {},\n",
    "        \"costs\": {},\n",
    "        \"section_titles\": structure, \n",
    "        \"writings\": {}\n",
    "    }\n",
    "    total = len(writing_plan)\n",
    "    for key, section in writing_plan.items():\n",
    "        print(f\"Writing '{section['section_title']}'...\")\n",
    "        if key in ['sec_1', f'sec_{total}']:\n",
    "            body_context = False\n",
    "        else:\n",
    "            body_context = True\n",
    "        \n",
    "        message = instruct(keyword, section, structure, vectorstore, body_context=body_context)\n",
    "        \n",
    "        # Retrieve response from ChatGPT\n",
    "        start_time = time.time() \n",
    "        \n",
    "        response = chatgpt(message)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        duration = end_time-start_time\n",
    "        \n",
    "        # Extract details from the response\n",
    "        content = response['choices'][0]['message']['content']\n",
    "        cost = response['usage']\n",
    "        print(f'Done. {duration:.2f} seconds. \\nCost:\\n{cost}\\n\\nResult (length: {len(content)}): {content[:50]}...{content[-20:]}\\n\\n')\n",
    "\n",
    "        # Update output with details\n",
    "        output[\"messages\"][key] = message\n",
    "        output[\"responses\"][key] = response\n",
    "        output[\"costs\"][key] = cost\n",
    "        \n",
    "        # Check and handle possible JSON format error\n",
    "        try:\n",
    "            output[\"writings\"][key] = json.loads(content, strict=False)['writing']\n",
    "        except json.JSONDecodeError:\n",
    "            output[\"writings\"][key] = content\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2cfe2f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'Introduction to the Premier League'...\n",
      "Done. 2.70 seconds. \n",
      "Cost:\n",
      "{\n",
      "  \"prompt_tokens\": 1306,\n",
      "  \"completion_tokens\": 70,\n",
      "  \"total_tokens\": 1376\n",
      "}\n",
      "\n",
      "Result (length: 350): {\n",
      "  \"writing\": \"In this blog post, we will explore...es it so special.\"\n",
      "}\n",
      "\n",
      "\n",
      "Writing 'Key Facts and Figures'...\n",
      "Done. 8.68 seconds. \n",
      "Cost:\n",
      "{\n",
      "  \"prompt_tokens\": 1312,\n",
      "  \"completion_tokens\": 255,\n",
      "  \"total_tokens\": 1567\n",
      "}\n",
      "\n",
      "Result (length: 1228): {\n",
      "  \"writing\": \"The Premier League, also known as ...iverpool with 1.\" \n",
      "}\n",
      "\n",
      "\n",
      "Writing 'Top Contenders of the Current Season'...\n",
      "Done. 5.00 seconds. \n",
      "Cost:\n",
      "{\n",
      "  \"prompt_tokens\": 1310,\n",
      "  \"completion_tokens\": 251,\n",
      "  \"total_tokens\": 1561\n",
      "}\n",
      "\n",
      "Result (length: 1279): {\n",
      "  \"writing\": \"The Premier League is renowned for...ce for the title.\"\n",
      "}\n",
      "\n",
      "\n",
      "Writing 'Exciting Matches and Memorable Moments'...\n",
      "Done. 10.69 seconds. \n",
      "Cost:\n",
      "{\n",
      "  \"prompt_tokens\": 1308,\n",
      "  \"completion_tokens\": 320,\n",
      "  \"total_tokens\": 1628\n",
      "}\n",
      "\n",
      "Result (length: 1711): {\"writing\": \"The Premier League is a thrilling and... across the globe.\"}\n",
      "\n",
      "\n",
      "Writing 'The Impact of Premier League on English Football'...\n",
      "Done. 10.30 seconds. \n",
      "Cost:\n",
      "{\n",
      "  \"prompt_tokens\": 1402,\n",
      "  \"completion_tokens\": 319,\n",
      "  \"total_tokens\": 1721\n",
      "}\n",
      "\n",
      "Result (length: 1840): {\"writing\": \"The Premier League has had a profound...ternational stage.\"}\n",
      "\n",
      "\n",
      "Writing 'Player Spotlight: Rising Stars and Legends'...\n",
      "Done. 6.32 seconds. \n",
      "Cost:\n",
      "{\n",
      "  \"prompt_tokens\": 1369,\n",
      "  \"completion_tokens\": 182,\n",
      "  \"total_tokens\": 1551\n",
      "}\n",
      "\n",
      "Result (length: 958): {\n",
      "  \"writing\": \"The Premier League has been a bree...football history.\"\n",
      "}\n",
      "\n",
      "\n",
      "Writing 'Conclusion: The Premier League Phenomenon'...\n",
      "Done. 6.18 seconds. \n",
      "Cost:\n",
      "{\n",
      "  \"prompt_tokens\": 1332,\n",
      "  \"completion_tokens\": 176,\n",
      "  \"total_tokens\": 1508\n",
      "}\n",
      "\n",
      "Result (length: 964): {\"writing\": \"As we reach the conclusion of our blo...of fans worldwide.\"}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "writing_output = expand_writing(trend, wiki, news, vectorstore, plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "420c503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_touch(writing_output):\n",
    "\n",
    "    # put the article together    \n",
    "    writings = [value for _, value in writing_output['writings'].items()]\n",
    "    titles = [value for value in writing_output['section_titles']]\n",
    "\n",
    "    parts = []\n",
    "    for writing, title in zip(writings, titles):\n",
    "        parts.append(f\"{title}\\n{writing}\")\n",
    "    raw_article = \"\\n\\n\".join(parts)  \n",
    "    \n",
    "\n",
    "    # improve\n",
    "    \n",
    "    instruction = \"\"\"People are searching \"{keyword}\" like crazy today, so I wrote a blog post for the topic.\n",
    "                Help me revise my blog post: \n",
    "                1. makesure appropriate tone, like a human talking \n",
    "                2. well-structured and coherent\n",
    "                3. readable and interesting\n",
    "                4. add emotions where possible\n",
    "                5. add professionalism where possible\n",
    "                6. add numbers where possible\"\"\"\n",
    "    \n",
    "    context = f\"\"\"my blog post:\\n\"{raw_article}\" \"\"\"\n",
    "    \n",
    "    message = [\n",
    "            {\"role\": \"system\", \"content\": \"\"\"You are a very helpful and passionate writer. \n",
    "                                            Keep in mind: \n",
    "                                            1. answer precise and concise\n",
    "                                            2. user has known you are a language model\n",
    "                                            3. always answer JSON object\"\"\"},\n",
    "            {\"role\": \"user\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": context},\n",
    "            {\"role\": \"user\", \"content\": \"\"\"output JSON object: {\"blog_title\": \"\",\"blog_content\": \"\", \"word_count\":\"\"}\"\"\"}\n",
    "        ]\n",
    "    \n",
    "    # getting response from ChatGPT\n",
    "    start_time = time.time() \n",
    "    try:\n",
    "        print(\"Starting...\")\n",
    "        response = chatgpt(message, max_tokens=2000)\n",
    "    except openai.error.APIConnectionError as e:\n",
    "        print(\"ChatGPT Disconnected\", e)\n",
    "        print('trying again')\n",
    "        response = chatgpt(message)\n",
    "\n",
    "    end_time = time.time()\n",
    "    running_time = end_time - start_time\n",
    "    print(f\"- Finished! | time: {running_time:.5f} seconds - \")\n",
    "    \n",
    "    # save outputs\n",
    "    content = response['choices'][0]['message']['content']\n",
    "    cost = response['usage']\n",
    "    print(\"- Cost tokens:\\n\", cost)\n",
    "\n",
    "    try:\n",
    "        json_content = json.loads(content, strict=False)\n",
    "        blog_title = json_content['blog_title']\n",
    "        blog_content = json_content['blog_content']\n",
    "        word_count = json_content['word_count']\n",
    "        blog_post = \"\\n\\n\".join([blog_title, blog_content])\n",
    "        output = {\n",
    "            \"message\": message, \n",
    "            \"response\": response,\n",
    "            \"cost\": cost,\n",
    "            \"word_count\": word_count,\n",
    "            \"answer\": blog_post\n",
    "        }\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Content is NOT JSON!!!!\")\n",
    "        output = {\n",
    "            \"message\": message, \n",
    "            \"response\": response,\n",
    "            \"cost\": cost,\n",
    "            \"answer\": content\n",
    "        }\n",
    "\n",
    "    return output    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8276dce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Finished! | time: 53.02400 seconds - \n",
      "- Cost tokens:\n",
      " {\n",
      "  \"prompt_tokens\": 1759,\n",
      "  \"completion_tokens\": 1631,\n",
      "  \"total_tokens\": 3390\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "blog_post = final_touch(writing_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9974b9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Premier League Phenomenon\n",
      "\n",
      "In this blog post, we will explore the exciting world of the Premier League, a popular and highly competitive soccer league in England. With its rich history and passionate fanbase, the Premier League has become a global phenomenon. Let's dive into the fascinating world of the Premier League and discover what makes it so special.\n",
      "\n",
      "Key Facts and Figures\n",
      "The Premier League, also known as The Football Association Premier League, is the highest level of the English football league system. It was founded on February 20, 1992, after the decision of First Division clubs to break away from the English Football League. The league consists of 20 clubs and operates on a system of promotion and relegation with the English Football League (EFL). Each team plays 38 matches, both home and away, during the season which typically runs from August to May. Matches are mostly played on weekends, with occasional weekday evening fixtures.\n",
      "\n",
      "The Premier League holds the top spot in the UEFA coefficient rankings, based on performances in European competitions, surpassing Spain's La Liga. English clubs have won a total of fifteen European championships, making the Premier League the second-highest producer of European Cup/UEFA Champions League titles. Since its inception in 1992, the league has witnessed competition from a total of 51 clubs, with seven of them winning the title. Manchester United boasts the most titles with 13, followed by Manchester City with 7, Chelsea with 5, Arsenal with 3, Blackburn Rovers with 1, Leicester City with 1, and Liverpool with 1.\n",
      "\n",
      "Top Contenders of the Current Season\n",
      "The Premier League is renowned for its fierce competition and intense battles for the title. As the 2023-24 season kicks off, all eyes are on the top contenders vying for glory. The defending champions, Manchester City, are once again poised for success. With key acquisitions of Croatian stars Josko Gvardiol and Mateo Kovacic, City is determined to make history as the first club to win four consecutive English crowns. Their dominance in recent years is a daunting prospect for the rest of the league.\n",
      "\n",
      "Meanwhile, Liverpool, having finished fifth last season and missing out on the Champions League, will be solely focused on securing their rank in Europe's top club competition. Their improved squad, bolstered by the additions of World Cup-winning midfielder Alexis Mac Allister and playmaker Dominik Szoboszlai, brings promising prospects for the Reds. Another team to watch out for is Arsenal. After a five-year absence, the Gunners have finally qualified for the Champions League, putting them in a unique position as they aim to balance their domestic campaign with European commitments. With these top contenders and several other determined teams, the current Premier League season promises unbeatable excitement and a thrilling race for the title.\n",
      "\n",
      "Exciting Matches and Memorable Moments\n",
      "The Premier League is a thrilling and action-packed competition, filled with exciting matches and unforgettable moments. Fans have been treated to astonishing comebacks, nail-biting finishes, and memorable displays of skill and talent. From historic rivalries like Manchester United vs. Liverpool to underdog triumphs like Leicester City's fairytale title win, the league has provided a treasure trove of unforgettable soccer experiences.\n",
      "\n",
      "One iconic match that stands out is the 2012 clash between Manchester City and Queens Park Rangers, on the final day of the season. With the title hanging in the balance, Sergio Aguero scored a dramatic last-minute goal, securing the championship for City in a heart-stopping fashion. Another moment etched into Premier League history is Steven Gerrard's slip in 2014, which ultimately cost Liverpool the title. These are just a few examples of the thrilling matches and memorable moments that make the Premier League a must-watch for football enthusiasts.\n",
      "\n",
      "Whether it's the relentless intensity of the Manchester derbies, the tactical battles between top managers, or the emergence of young talents like Phil Foden and Mason Mount, the Premier League continually delivers moments that captivate the world. It has had a profound impact on English football, attracting a global audience and generating billions in revenue. The league's competitive nature and high-quality football have elevated the standards of the game in England and helped English clubs become a force to be reckoned with in European competitions. The Premier League is not just a football league, it's a phenomenon that has revolutionized the sport and captivated fans across the globe.\n",
      "\n",
      "The Impact of Premier League on English Football\n",
      "The Premier League has had a profound impact on English football, revolutionizing the sport in multiple ways. One significant influence is the league's contribution to grassroots football. The Premier League's popularity and success have inspired a new generation of aspiring young footballers, leading to increased participation at the grassroots level. This surge in interest has resulted in improved facilities, better coaching, and enhanced youth development programs across the country.\n",
      "\n",
      "Furthermore, the Premier League's financial power has allowed clubs to invest heavily in their youth academies, nurturing and developing talented players. This investment has not only benefited the clubs but has also had a positive impact on the national team. Many of England's top players have emerged from Premier League academies, showcasing the league's vital role in producing and nurturing talent.\n",
      "\n",
      "Moreover, the Premier League's global reach and exposure have elevated the sport's popularity within England. The league's exciting brand of football, filled with thrilling matches and memorable moments, has captivated fans both domestically and internationally. The Premier League's revenue and marketing strategies have also helped enhance the overall image and commercial success of English football, attracting top players from around the world and providing a platform for English clubs to compete on the global stage.\n",
      "\n",
      "In conclusion, the Premier League's impact on English football cannot be overstated. It has transformed the sport at all levels, from grassroots to elite, and has played a crucial role in shaping the nation's footballing culture. The league's success, popularity, and financial power have contributed to the growth and development of English football, both domestically and on the international stage.\n",
      "\n",
      "Player Spotlight: Rising Stars and Legends\n",
      "The Premier League has been a breeding ground for both rising stars and footballing legends. From young prospects making a name for themselves to seasoned veterans leaving an indelible mark, the league has celebrated the individuals who have made it truly extraordinary. Each season, new talents emerge, capturing the attention and admiration of fans around the world. Players like Phil Foden, Mason Mount, and Bukayo Saka have proven themselves to be the future of English football, showcasing their skills and contributing to the success of their respective clubs. On the other hand, legends of the game such as Cristiano Ronaldo, Sergio Aguero, and Steven Gerrard have graced the Premier League with their brilliance, captivating audiences with their goal-scoring prowess and unmatched passion for the game. The Premier League continues to be a platform where rising stars can shine and legends can etch their names in football history.\n",
      "\n",
      "Conclusion: The Premier League Phenomenon\n",
      "As we reach the conclusion of our blog post, it is clear that the Premier League is truly a phenomenon in the world of football. With its passionate fans, thrilling matches, and rich history, the Premier League has become the pinnacle of English football. The 2023-24 season promises exciting competition, with defending champions Manchester City aiming for a historic fourth consecutive title. However, the Premier League is not just about the top contenders and the intense battles on the pitch. It is about the impact it has on English football as a whole, from grassroots development to the global recognition it brings. Moreover, the Premier League showcases both rising stars and legendary players, captivating fans with their exceptional skills and performances. As we celebrate the brilliance of English football, let us acknowledge the enduring legacy of the Premier League and its unmatched ability to captivate millions of fans worldwide.\n"
     ]
    }
   ],
   "source": [
    "print(blog_post['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f645e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
